{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Plotting reliability diagrams\n\nThis example illustrates how to visualise the reliability diagram for a binary\nprobabilistic classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Miquel Perello Nieto <miquel.perellonieto@bristol.ac.uk>\n# License: new BSD\n\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example shows different ways to visualise the reliability diagram for a\nbinary classification problem.\n\nFirst we will generate two synthetic models and some synthetic scores and\nlabels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nnp.random.seed(42)\n\nn_c1 = n_c2 = 500\np = np.concatenate((np.random.beta(2, 5, n_c1),\n                    np.random.beta(4, 3, n_c2)\n                   ))\n\ny = np.concatenate((np.zeros(n_c1), np.ones(n_c2)))\n\ns1 = 1/(1 + np.exp(-8*(p - 0.5)))\ns2 = 1/(1 + np.exp(-3*(p - 0.5)))\n\nplt.scatter(s1, p, label='Model 1')\nplt.scatter(s2, p, label='Model 2')\nplt.scatter(p, y)\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlabel('Model scores')\nplt.ylabel('Sample true probability')\nplt.grid()\nplt.legend()\n\np = np.vstack((1 - p, p)).T\ns1 = np.vstack((1 - s1, s1)).T\ns2 = np.vstack((1 - s2, s2)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A perfect calibration should be as follows, compared with the generated\nscores\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n\np_g_p = stats.beta.pdf(x=p[:, 1], a=3, b=2)\np_g_n = stats.beta.pdf(x=p[:, 1], a=2, b=7)\n\np_hat = p_g_p/(p_g_n+p_g_p)\np_hat = np.vstack((1 - p_hat, p_hat)).T\n\nplt.scatter(p[:, 1], s1[:, 1], label='Model 1')\nplt.scatter(p[:, 1], s2[:, 1], label='Model 2')\nplt.scatter(p[:, 1], p_hat[:, 1], color='red', label='Bayes optimal correction')\nplt.xlabel('Sample true probability')\nplt.ylabel('Model scores')\nplt.grid()\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are at least 2 very common ways to show a reliability diagram for a\nprobabilistic binary classifier. Drawing a line between all the binned mean\npredictions and the true proportion of positives.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pycalib.visualisations import plot_reliability_diagram\n\nfig = plot_reliability_diagram(labels=y, scores=s1, show_histogram=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And showing bars instead of a lineplot, usually with errorbars showing the\ndiscrepancy with respect to a perfectly calibrated model (diagonal)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=s1,\n                               class_names=['Negative', 'Positive'],\n                               show_gaps=True, show_bars=True,\n                               show_histogram=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, both previous illustrations do not include the number of samples\nthat fall into each bin. By default the parameter show_bars is set to True as\nthis information is crucial to understand how reliable is each estimation,\nand how this affects some of the calibration metrics.\nWe also specify the bin boundaries and change the color of the gaps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=s1,\n                               class_names=['Negative', 'Positive'],\n                               show_gaps=True, color_gaps='firebrick',\n                               bins=[0, .3, .4, .45, .5, .55, .6, .7, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also common to plot only the confidence (considering the winning class\nonly as positive class for each prediction). Notice that the class names is\nautomatically set to *winning* class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=s1,\n                               show_gaps=True,\n                               confidence=True,\n                               show_bars=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can enable some parameters to show several aspects of the reliability\ndiagram. For example, we can add a histogram indicating the number of samples\non each bin (or show the count in each marker), the correction that should be\napplied to the average scores in order to calibrate the model can be also\nshown as red arrows pointing to the direction of the diagonal (perfectly\ncalibrated model). And even the true class of each sample at the y\ncoordinates [0 and 1] for each scored instance (50% of the data in\nthis example, but default is 100%).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=s1,\n                               legend=['Model 1'],\n                               show_histogram=True,\n                               bins=9, class_names=['Negative', 'Positive'],\n                               show_counts=True,\n                               show_correction=True,\n                               sample_proportion=0.5,\n                               hist_per_class=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be also useful to have 95% confidence intervals for each bin by\nperforming a binomial proportion confidence interval with various statistical\ntests. This function uses https://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportion_confint.html\nthus accepts the different tests available in the statsmodels library. In the\nfollowing example we use the Clopper-Pearson interval based on Beta\ndistribution and a confidence interval of 95%.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=s2,\n                               legend=['Model 2'],\n                               show_histogram=True,\n                               show_counts=True,\n                               bins=13, class_names=['Negative', 'Positive'],\n                               sample_proportion=1.0,\n                               errorbar_interval=0.95,\n                               interval_method='beta',\n                               color_list=['orange'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function also allows the visualisation of multiple models for comparison.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=[s1, s2],\n                               legend=['Model 1', 'Model 2'],\n                               show_histogram=True,\n                               bins=10, class_names=['Negative', 'Positive'],\n                               errorbar_interval=0.95,\n                               interval_method='beta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to draw reliability diagram for multiple classes as well. We\nwill simulate 3 classes by changing some original labels to a 3rd class, and\nmodifying the scores of Model 1 and 2 to create new models 3 and 4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class_2_idx = range(int(len(y)/3), int(2*len(y)/3))\ny[class_2_idx] = 2\ns1 = np.hstack((s1, s1[:, 1].reshape(-1, 1)))\ns1[class_2_idx,2] *= 3\ns1 /= s1.sum(axis=1)[:, None]\ns2 = np.hstack((s2, s2[:, 1].reshape(-1, 1)))\ns2[class_2_idx,2] *= 2\ns2 /= s2.sum(axis=1)[:, None]\n\nfig = plot_reliability_diagram(labels=y, scores=[s1, s2],\n                               legend=['Model 3', 'Model 4'],\n                               show_histogram=True,\n                               color_list=['darkgreen', 'chocolate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we are only interested in the confidence, the 3 classes can be visualised\nin a single reliability diagram\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=[s1, s2],\n                               legend=['Model 3', 'Model 4'],\n                               show_histogram=True,\n                               color_list=['darkgreen', 'chocolate'],\n                               confidence=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same can be done with the bars.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_reliability_diagram(labels=y, scores=s1,\n                               legend=['Model 3'],\n                               show_histogram=True,\n                               color_list=['darkgreen'],\n                               show_bars=True,\n                               show_gaps=True,\n                               color_gaps='orange')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have precomputed the average proportion of true positives and\npredictions, or we have access to the ground truth, it is possible to plot\nthe same reliability diagram using the following function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pycalib.visualisations import plot_reliability_diagram_precomputed\n\navg_true = [np.array([.1, .3, .6, .8, .9, 1]).reshape(-1, 1),\n            np.array([.2, .4, .5, .7, .8, .9]).reshape(-1, 1)]\navg_pred = [np.array([.01, .25, .4, .6, .7, .8]).reshape(-1, 1),\n            np.array([.15, .39, .7, .75, .8, .9]).reshape(-1, 1)]\n\nfig = plot_reliability_diagram_precomputed(avg_true, avg_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly for a multiclass problem we can provide full matrices of size\n(n_bins, n_classes) instead. Notice that the order of the predicted scores\ndoesn't need to be in order, and the probabilities doesn't need to sum to one\namong all classes, as the way they are computed may be from different\ninstances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "avg_true = [np.array([[.1, .3, .6, .8, .9, 1.],\n                      [.0, .2, .4, .7, .8, .9],\n                      [.1, .2, .3, .5, .6, .8]]).T,\n            np.array([[.1, .4, .7, .8, .9, 1.],\n                      [.9, .3, .8, .2, .7, .1],\n                      [.2, .3, .5, .4, .7, .1]]).T]\navg_pred = [np.array([[.0, .3, .6, .7, .8, 9.],\n                      [.1, .2, .3, .5, .8, .7],\n                      [.3, .5, .4, .7, .8, .9]]).T,\n            np.array([[.0, .3, .6, .8, .9, 1.],\n                      [.8, .1, .6, .2, .9, 0.],\n                      [.1, .4, .6, .3, .5, 0.]]).T]\n\nfig = plot_reliability_diagram_precomputed(avg_true, avg_pred)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}