
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/visualisations/xmpl_reliability_diagram.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_visualisations_xmpl_reliability_diagram.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_visualisations_xmpl_reliability_diagram.py:


=============================
Plotting reliability diagrams
=============================

This example illustrates how to visualise the reliability diagram for a binary
probabilistic classifier.

.. GENERATED FROM PYTHON SOURCE LINES 9-15

.. code-block:: default

    # Author: Miquel Perello Nieto <miquel.perellonieto@bristol.ac.uk>
    # License: new BSD

    print(__doc__)
    SAVEFIGS=False








.. GENERATED FROM PYTHON SOURCE LINES 16-21

This example shows different ways to visualise the reliability diagram for a
binary classification problem.

First we will generate two synthetic models and some synthetic scores and
labels.

.. GENERATED FROM PYTHON SOURCE LINES 21-52

.. code-block:: default


    import matplotlib.pyplot as plt
    import numpy as np
    np.random.seed(42)

    n_c1 = n_c2 = 200
    p = np.concatenate((np.random.beta(2, 5, n_c1),
                        np.random.beta(4, 3, n_c2)
                       ))

    y = np.concatenate((np.zeros(n_c1), np.ones(n_c2)))

    print(p.shape)
    print(y.shape)

    s1 = 1/(1 + np.exp(-3*(p - 0.5)))
    s2 = 1/(1 + np.exp(-8*(p - 0.5)))

    plt.scatter(s1, p, label='Model 1')
    plt.scatter(s2, p, label='Model 2')
    plt.scatter(p, y)
    plt.plot([0, 1], [0, 1], 'r--')
    plt.xlabel('Model scores')
    plt.ylabel('Sample true probability')
    plt.grid()
    plt.legend()

    p = np.vstack((1 - p, p)).T
    s1 = np.vstack((1 - s1, s1)).T
    s2 = np.vstack((1 - s2, s2)).T




.. image:: /examples/visualisations/images/sphx_glr_xmpl_reliability_diagram_001.png
    :alt: xmpl reliability diagram
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (400,)
    (400,)




.. GENERATED FROM PYTHON SOURCE LINES 53-55

A perfect calibration should be as follows, compared with the generated
scores

.. GENERATED FROM PYTHON SOURCE LINES 55-72

.. code-block:: default


    import scipy.stats as stats

    p_g_p = stats.beta.pdf(x=p[:, 1], a=3, b=2)
    p_g_n = stats.beta.pdf(x=p[:, 1], a=2, b=7)

    p_hat = p_g_p/(p_g_n+p_g_p)
    p_hat = np.vstack((1 - p_hat, p_hat)).T

    plt.scatter(p[:, 1], s1[:, 1], label='Model 1')
    plt.scatter(p[:, 1], s2[:, 1], label='Model 2')
    plt.scatter(p[:, 1], p_hat[:, 1], color='red', label='Bayes optimal correction')
    plt.xlabel('Sample true probability')
    plt.ylabel('Model scores')
    plt.grid()
    plt.legend()




.. image:: /examples/visualisations/images/sphx_glr_xmpl_reliability_diagram_002.png
    :alt: xmpl reliability diagram
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7fd7065a9280>



.. GENERATED FROM PYTHON SOURCE LINES 73-74

Then we can show the most common form to visualise a reliability diagram

.. GENERATED FROM PYTHON SOURCE LINES 74-83

.. code-block:: default


    from pycalib.visualisations import plot_reliability_diagram

    fig = plot_reliability_diagram(labels=y, scores_list=[s1, ],
                                   legend=['Model 1'],
                                   class_names=['Negative', 'Positive'])
    if SAVEFIGS:
        fig.savefig('fig1.png')




.. image:: /examples/visualisations/images/sphx_glr_xmpl_reliability_diagram_003.png
    :alt: xmpl reliability diagram
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:91: RuntimeWarning: invalid value encountered in true_divide
      avg_true = np.divide(bin_true, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:92: RuntimeWarning: invalid value encountered in true_divide
      avg_pred = np.divide(bin_pred, bin_total)




.. GENERATED FROM PYTHON SOURCE LINES 84-91

We can enable some parameters to show several aspects of the reliability
diagram. For example, we can add a histogram indicating the number of samples
on each bin (or show the count in each marker), the correction that should be
applied to the average scores in order to calibrate the model can be also
shown as red arrows pointing to the direction of the diagonal (perfectly
calibrated model). And even the true class of each sample at the y
coordinates [0 and 1] for each scored instance.

.. GENERATED FROM PYTHON SOURCE LINES 91-106

.. code-block:: default


    from pycalib.visualisations import plot_reliability_diagram

    fig = plot_reliability_diagram(labels=y, scores_list=[s1, ],
                                   legend=['Model 1'],
                                   show_histogram=True,
                                   bins=9, class_names=['Negative', 'Positive'],
                                   show_counts=True,
                                   show_correction=True,
                                   show_samples=True,
                                   sample_proportion=1.0,
                                   hist_per_class=True)
    if SAVEFIGS:
        fig.savefig('fig2.png')




.. image:: /examples/visualisations/images/sphx_glr_xmpl_reliability_diagram_004.png
    :alt: xmpl reliability diagram
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:91: RuntimeWarning: invalid value encountered in true_divide
      avg_true = np.divide(bin_true, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:92: RuntimeWarning: invalid value encountered in true_divide
      avg_pred = np.divide(bin_pred, bin_total)




.. GENERATED FROM PYTHON SOURCE LINES 107-113

It can be also useful to have 95% confidence intervals for each bin by
performing a binomial proportion confidence interval with various statistical
tests. This function uses https://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportion_confint.html
thus accepts the different tests available in the statsmodels library. In the
following example we use the Clopper-Pearson interval based on Beta
distribution and a confidence interval of 95%.

.. GENERATED FROM PYTHON SOURCE LINES 113-125

.. code-block:: default


    fig = plot_reliability_diagram(labels=y, scores_list=[s2, ],
                                   legend=['Model 2'],
                                   show_histogram=False,
                                   show_counts=True,
                                   bins=13, class_names=['Negative', 'Positive'],
                                   show_samples=True, sample_proportion=1.0,
                                   errorbar_interval=0.95,
                                   interval_method='beta',)
    if SAVEFIGS:
        fig.savefig('fig3.png')




.. image:: /examples/visualisations/images/sphx_glr_xmpl_reliability_diagram_005.png
    :alt: xmpl reliability diagram
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 126-127

The function also allows the visualisation of multiple models for comparison.

.. GENERATED FROM PYTHON SOURCE LINES 127-138

.. code-block:: default


    fig = plot_reliability_diagram(labels=y, scores_list=[s1, s2],
                                   legend=['Model 1', 'Model 2'],
                                   show_histogram=True,
                                   bins=10, class_names=['Negative', 'Positive'],
                                   errorbar_interval=0.95,
                                   interval_method='beta')
    if SAVEFIGS:
        fig.savefig('fig4.png')





.. image:: /examples/visualisations/images/sphx_glr_xmpl_reliability_diagram_006.png
    :alt: xmpl reliability diagram
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:91: RuntimeWarning: invalid value encountered in true_divide
      avg_true = np.divide(bin_true, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:92: RuntimeWarning: invalid value encountered in true_divide
      avg_pred = np.divide(bin_pred, bin_total)
    /opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/statsmodels/stats/proportion.py:90: RuntimeWarning: invalid value encountered in true_divide
      q_ = count * 1. / nobs




.. GENERATED FROM PYTHON SOURCE LINES 139-143

It is possible to draw reliability diagram for multiple classes as well. In
this case we will just assign 1/3 of the samples to a 3rd class, will
give the same scores as some of the other classes, and normalise them to sum
to one.

.. GENERATED FROM PYTHON SOURCE LINES 143-158

.. code-block:: default


    class_2_idx = range(int(len(y)/3), int(2*len(y)/3))
    y[class_2_idx] = 2
    s1 = np.hstack((s1, s1[:, 1].reshape(-1, 1)))
    s1[class_2_idx,2] *= 3
    s1 /= s1.sum(axis=1)[:, None]
    s2 = np.hstack((s2, s2[:, 1].reshape(-1, 1)))
    s2[class_2_idx,2] *= 2

    fig = plot_reliability_diagram(labels=y, scores_list=[s1, s2],
                                   legend=['Model 1', 'Model 2'],
                                   show_histogram=True,
                                   )
    if SAVEFIGS:
        fig.savefig('fig5.png')



.. image:: /examples/visualisations/images/sphx_glr_xmpl_reliability_diagram_007.png
    :alt: xmpl reliability diagram
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:91: RuntimeWarning: invalid value encountered in true_divide
      avg_true = np.divide(bin_true, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:92: RuntimeWarning: invalid value encountered in true_divide
      avg_pred = np.divide(bin_pred, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:91: RuntimeWarning: invalid value encountered in true_divide
      avg_true = np.divide(bin_true, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:92: RuntimeWarning: invalid value encountered in true_divide
      avg_pred = np.divide(bin_pred, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:91: RuntimeWarning: invalid value encountered in true_divide
      avg_true = np.divide(bin_true, bin_total)
    /home/runner/work/PyCalib/PyCalib/pycalib/visualisations/__init__.py:92: RuntimeWarning: invalid value encountered in true_divide
      avg_pred = np.divide(bin_pred, bin_total)





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  2.045 seconds)


.. _sphx_glr_download_examples_visualisations_xmpl_reliability_diagram.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/perellonieto/pycalib/gh-pages?filepath=notebooks/examples/visualisations/xmpl_reliability_diagram.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: xmpl_reliability_diagram.py <xmpl_reliability_diagram.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: xmpl_reliability_diagram.ipynb <xmpl_reliability_diagram.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
