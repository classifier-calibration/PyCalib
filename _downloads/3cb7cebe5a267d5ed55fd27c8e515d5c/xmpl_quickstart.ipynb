{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Quickstart\n\nThis example shows a simple comparison of the expected calibration error of a\nnon-calibrated method against a calibrated method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Miquel Perello Nieto <miquel.perellonieto@bristol.ac.uk>\n# License: new BSD\n\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First choose a classifier\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n\nclf = GaussianNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And a dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(\n    n_samples=100000, n_features=20, n_informative=4, n_redundant=4,\n    random_state=42\n)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see how calibrated it is after training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf.fit(X_train, Y_train)\n\nn_correct = sum(clf.predict(X_test) == Y_test)\nn_test = Y_test.shape[0]\n\nprint(f\"The classifier gets {n_correct} correct \"\n      f\"predictions out of {n_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can asses the confidence expected calibration error\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pycalib.metrics import conf_ECE\n\nscores = clf.predict_proba(X_test)\ncece = conf_ECE(Y_test, scores, bins=15)\n\nprint(f\"The classifier gets a confidence expected \"\n      f\"calibration error of {cece:0.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at its reliability diagram\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pycalib.visualisations import plot_reliability_diagram\n\nplot_reliability_diagram(labels=Y_test, scores=scores, show_histogram=True,\n                         show_bars=True, show_gaps=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see how a calibration can improve the conf-ECE\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pycalib.models import IsotonicCalibration\ncal = IsotonicCalibration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can put together a probabilistic classifier with the chosen calibration\nmethod\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pycalib.models import CalibratedModel\n\ncal_clf = CalibratedModel(base_estimator=clf, calibrator=cal,\n                          fit_estimator=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can train both classifier and calibrator all together.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cal_clf.fit(X_train, Y_train)\nn_correct = sum(cal_clf.predict(X_test) == Y_test)\n\nprint(f\"The calibrated classifier gets {n_correct} \"\n      f\"correct predictions out of {n_test}\")\n\nscores_cal = cal_clf.predict_proba(X_test)\ncece = conf_ECE(Y_test, scores_cal, bins=15)\n\nprint(f\"The calibrated classifier gets a confidence \"\n      f\"expected calibration error of {cece:0.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can train both classifier and calibrator all together.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pycalib.visualisations import plot_reliability_diagram\n\nplot_reliability_diagram(labels=Y_test, scores=scores_cal, show_histogram=True,\n                         show_bars=True, show_gaps=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}