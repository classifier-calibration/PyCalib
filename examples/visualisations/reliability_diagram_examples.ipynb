{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solar-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "overhead-static",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_c1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2ee608d84e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m p = np.concatenate((np.random.beta(2, 7, n_c1),\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_c2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_c1' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "p = np.concatenate((np.random.beta(2, 7, n_c1),\n",
    "                    np.random.beta(3, 2, n_c2)\n",
    "                   ))\n",
    "p_g_p = stats.beta.pdf(x=p, a=3, b=2)\n",
    "p_g_n = stats.beta.pdf(x=p, a=2, b=7)\n",
    "\n",
    "p_hat = p_g_p/(p_g_n+p_g_p)\n",
    "\n",
    "p_hat = np.vstack((1 - p_hat, p_hat)).T\n",
    "#plt.hist(p_hat)\n",
    "\n",
    "plt.scatter(p_hat[:, 1], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_c1 = n_c2 = 500\n",
    "p = np.concatenate((np.random.beta(2, 7, n_c1),\n",
    "                    np.random.beta(3, 2, n_c2)\n",
    "                   ))\n",
    "\n",
    "calibrated_model = np.random.beta(1, 5, n_c1)\n",
    "\n",
    "y = np.concatenate((np.zeros(n_c1), np.ones(n_c2)))\n",
    "\n",
    "print(p.shape)\n",
    "print(y.shape)\n",
    "\n",
    "s1 = 1/(1 + np.exp(-3*(p - 0.5)))\n",
    "s2 = 1/(1 + np.exp(-8*(p - 0.5)))\n",
    "\n",
    "plt.scatter(p, s1, label='Model 1')\n",
    "plt.scatter(p, s2, label='Model 2')\n",
    "plt.scatter(p, y)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "p = np.vstack((1 - p, p)).T\n",
    "s1 = np.vstack((1 - s1, s1)).T\n",
    "s2 = np.vstack((1 - s2, s2)).T\n",
    "\n",
    "print(p.shape)\n",
    "print(s1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.calibration import calibration_curve\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "def plot_reliability_diagram(labels, scores_list, legend, show_histogram=True,\n",
    "                             bins=10, class_names=None, fig=None,\n",
    "                             show_counts=False, errorbar_interval=None,\n",
    "                             interval_method='beta', fmt='s-',\n",
    "                             show_correction=False,\n",
    "                             show_samples=False,\n",
    "                             sample_proportion=1.0,\n",
    "                             hist_per_class=False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ==========\n",
    "    labels : array (n_samples, )\n",
    "        Labels indicating the ground class\n",
    "    scores_list : list of matrices [(n_samples, n_classes)]\n",
    "        Output probability scores for every method\n",
    "    legend : list of strings\n",
    "        Text to use for the legend\n",
    "    n_bins : int\n",
    "        Number of bins to create in the scores' space\n",
    "    histogram : boolean\n",
    "        If True, it generates an additional figure showing the number of\n",
    "        samples in each bin.\n",
    "\n",
    "    Regurns\n",
    "    =======\n",
    "    fig : matplotlib.pyplot.figure\n",
    "        Figure with the reliability diagram\n",
    "    fig2 : matplotlib.pyplot.figure\n",
    "        Only if histogram == True\n",
    "    '''\n",
    "    classes = np.unique(labels)\n",
    "    n_classes = len(classes)\n",
    "    labels = label_binarize(labels, classes=classes)\n",
    "\n",
    "    if class_names is None:\n",
    "        if n_classes == 2:\n",
    "            class_names = ['2']\n",
    "        else:\n",
    "            class_names = [str(i+1) for i in range(n_classes)]\n",
    "            \n",
    "    if n_classes == 2:\n",
    "        scores_list = [score[:, 1].reshape(-1, 1) for score in scores_list]\n",
    "        class_names = [class_names[1], ]\n",
    "\n",
    "    n_columns = labels.shape[1]\n",
    "    \n",
    "    if fig is None:\n",
    "        fig = plt.figure(figsize=(n_columns*4, 4))\n",
    "        \n",
    "    if show_histogram:\n",
    "        spec = gridspec.GridSpec(ncols=n_columns, nrows=2,\n",
    "                                 height_ratios=[5, 1],\n",
    "                                 wspace=0.01, hspace=0.04)\n",
    "    else:\n",
    "        spec = gridspec.GridSpec(ncols=1, nrows=1)\n",
    "        \n",
    "    if isinstance(bins, int):\n",
    "        n_bins = bins\n",
    "        bins = np.linspace(0, 1 + 1e-8, n_bins)\n",
    "    elif isinstance(bins, list) or isinstance(bins, np.ndarray):\n",
    "        n_bins = len(bins) - 1\n",
    "        \n",
    "    for i in range(n_columns):\n",
    "        ax1 = fig.add_subplot(spec[i])\n",
    "\n",
    "        for score, name in zip(scores_list, legend):\n",
    "            bin_idx = np.digitize(score[:, i], bins) - 1\n",
    "\n",
    "            bin_true = np.bincount(bin_idx, weights=labels[:, i], minlength=n_bins)\n",
    "            bin_pred = np.bincount(bin_idx, weights=score[:, i], minlength=n_bins)\n",
    "            bin_total = np.bincount(bin_idx, minlength=n_bins)\n",
    "            \n",
    "            avg_true = np.divide(bin_true, bin_total)\n",
    "            avg_pred = np.divide(bin_pred, bin_total)\n",
    "\n",
    "            if errorbar_interval is None:\n",
    "                p = ax1.plot(avg_pred, avg_true, fmt, label=name)\n",
    "                color = p[0].get_color()\n",
    "            else:\n",
    "                intervals = proportion_confint(count=bin_true, nobs=bin_total,\n",
    "                                               alpha=1-errorbar_interval,\n",
    "                                               method=interval_method)\n",
    "                intervals = np.array(intervals)\n",
    "                yerr = intervals - avg_true\n",
    "                yerr = np.abs(yerr)\n",
    "                ebar  = ax1.errorbar(avg_pred, avg_true, yerr=yerr,\n",
    "                                    label=name, fmt=fmt, markersize=5)\n",
    "                color = ebar[0].get_color()\n",
    "            \n",
    "            if show_counts:\n",
    "                for ap, at, count in zip(avg_pred, avg_true, bin_total):\n",
    "                    if np.isfinite(ap) and np.isfinite(at):\n",
    "                        ax1.text(ap, at, str(count), fontsize=8, ha='center', va='center',\n",
    "                                bbox=dict(boxstyle='square,pad=0.15', fc='white', \n",
    "                                          ec=color))\n",
    "                        \n",
    "            if show_correction:\n",
    "                for ap, at in zip(avg_pred, avg_true):\n",
    "                    ax1.arrow(ap, at, at - ap, 0, color='red', head_width=0.02,\n",
    "                             length_includes_head=True)\n",
    "                    \n",
    "            if show_samples:\n",
    "                idx = np.random.choice(labels.shape[0], int(sample_proportion*labels.shape[0]))\n",
    "                ax1.scatter(score[idx, i], labels[idx, i], marker='d', s=100,\n",
    "                           alpha=0.1)\n",
    "                \n",
    "        ax1.plot([0, 1], [0, 1], \"r--\")\n",
    "        ax1.set_xlim([0, 1])\n",
    "        ax1.set_ylim([0, 1])\n",
    "        #ax1.set_title('Class {}'.format(class_names[i]))\n",
    "        ax1.set_xlabel('Mean predicted value (Class {})'.format(\n",
    "            class_names[i]))\n",
    "        if i == 0:\n",
    "            ax1.set_ylabel('Fraction of positives')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        if show_histogram:\n",
    "            ax1.get_xaxis().set_visible(False)\n",
    "            lines = ax1.get_lines()\n",
    "            #ax2.set_xticklabels([])\n",
    "            ax2 = fig.add_subplot(spec[n_columns + i])\n",
    "            for j, (score, name) in enumerate(zip(scores_list, legend)):\n",
    "                color = lines[j].get_color()\n",
    "                if hist_per_class:\n",
    "                    for c in [0, 1]:\n",
    "                        linestyle = ('solid','dashed')[c]\n",
    "                        ax2.hist(score[labels[:, i]==c, i], range=(0, 1), bins=bins, label=name,\n",
    "                                 histtype=\"step\", lw=2, linestyle=linestyle, color=color)\n",
    "                else:\n",
    "                    ax2.hist(score[:, i], range=(0, 1), bins=bins, label=name,\n",
    "                             histtype=\"step\", lw=2)\n",
    "                ax2.set_xlim([0, 1])\n",
    "                ax2.set_xlabel('Mean predicted value (Class {})'.format(\n",
    "                    class_names[i]))\n",
    "                if i == 0:\n",
    "                    ax2.set_ylabel('#count')\n",
    "                ax2.grid(True)\n",
    "\n",
    "    lines, labels = fig.axes[0].get_legend_handles_labels()\n",
    "    fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0, 0, 1, 1),\n",
    "               bbox_transform=fig.transFigure, ncol=6)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "fig = plot_reliability_diagram(labels=y, scores_list=[p_hat, ], legend=['Model 2'],\n",
    "                               show_histogram=True,\n",
    "                               bins=10, class_names=['Negative', 'Positive'], fig=fig, show_counts=True,\n",
    "                               errorbar_interval=None, show_correction=False,\n",
    "                               show_samples=True, sample_proportion=1.0,\n",
    "                               hist_per_class=True, fmt='o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-diana",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = [0]\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig = plot_reliability_diagram(labels=y, scores_list=[s1, s2], legend=['Model 1', 'Model 2'],\n",
    "                               show_histogram=True,\n",
    "                               bins=10, class_names=['Negative', 'Positive'], fig=fig, show_counts=False,\n",
    "                               errorbar_interval=0.95, show_correction=False,\n",
    "                               show_samples=True, sample_proportion=0.5,\n",
    "                               hist_per_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-support",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-cookie",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
